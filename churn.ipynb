{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data set and rapid preview data\n",
    "<li><ul>7043 entries</ul></li>\n",
    "<li><ul>21 columns</ul></li>\n",
    "<li><ul>0 null value</ul></li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = 'churn/'\n",
    "df =pd.read_csv(path_data+'dataset-churn.csv')\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_NA(data):\n",
    "    \"\"\"\n",
    "    Returns a pandas dataframe denoting the total number of NA values and the percentage of NA values in each column.\n",
    "    The column names are noted on the index.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: dataframe\n",
    "    \"\"\"\n",
    "    # pandas series denoting features and the sum of their null values\n",
    "    null_sum = data.isnull().sum()# instantiate columns for missing data\n",
    "    total = null_sum.sort_values(ascending=False)\n",
    "    percent = ( ((null_sum / len(data.index))*100).round(2) ).sort_values(ascending=False)\n",
    "    \n",
    "    # concatenate along the columns to create the complete dataframe\n",
    "    df_NA = pd.concat([total, percent], axis=1, keys=['Number of NA', 'Percent NA'])\n",
    "    \n",
    "    # drop rows that don't have any missing data; omit if you want to keep all rows\n",
    "    df_NA = df_NA[ (df_NA.T != 0).any() ]\n",
    "    \n",
    "    return df_NA\n",
    "miss = assess_NA(df)\n",
    "miss.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change type of some columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set customerID as string\n",
    "def changeTypeCol(df):\n",
    "    df['customerID'] = df['customerID'].astype('str')\n",
    "    # set some columns to categorical\n",
    "    df['SeniorCitizen'] = df['SeniorCitizen'].astype('category')\n",
    "    df['gender'] = df['gender'].astype('category')\n",
    "    df['Partner'] = df['Partner'].astype('category')\n",
    "    df['Dependents'] = df['Dependents'].astype('category')\n",
    "    df['PhoneService'] = df['PhoneService'].astype('category')\n",
    "    df['MultipleLines'] = df['MultipleLines'].astype('category')\n",
    "    df['InternetService'] = df['InternetService'].astype('category')\n",
    "    df['OnlineSecurity'] = df['OnlineSecurity'].astype('category')\n",
    "    df['OnlineBackup'] = df['OnlineBackup'].astype('category')\n",
    "    df['DeviceProtection'] = df['DeviceProtection'].astype('category')\n",
    "    df['TechSupport'] = df['TechSupport'].astype('category')\n",
    "    df['StreamingTV'] = df['StreamingTV'].astype('category')\n",
    "    df['StreamingMovies'] = df['StreamingMovies'].astype('category')\n",
    "    df['Contract'] = df['Contract'].astype('category')\n",
    "    df['PaperlessBilling'] = df['PaperlessBilling'].astype('category')\n",
    "    df['PaymentMethod'] = df['PaymentMethod'].astype('category')\n",
    "    df['Churn'] = df['Churn'].astype('category')\n",
    "    #df['TotalCharges'] = df['TotalCharges'].astype('float64')\n",
    "    return df\n",
    "\n",
    "df = changeTypeCol(df)\n",
    "\"\"\"\n",
    "#handling mixed types of elements in a columns TotalCharges\n",
    "try:\n",
    "    df['TotalCharges'] = df['TotalCharges'][df['TotalCharges'].astype(float)] \n",
    "except:\n",
    "    df['TotalCharges'] = np.median(df['TotalCharges'])\n",
    "df.shape\n",
    "#miss = assess_NA(df)\n",
    "#miss.head()\n",
    "\"\"\"\n",
    "\n",
    "df[\"TotalCharges\"] = df[\"TotalCharges\"].convert_objects(convert_numeric=True).fillna(0)\n",
    "#df.info()\n",
    "print(df[\"TotalCharges\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace TotalCharges empty to median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df[\"TotalCharges\"] = df[\"TotalCharges\"].replace(np.median(df[\"TotalCharges\"]), \"\")\n",
    "print(df.isnull().sum())\n",
    "df.to_csv(path_data+\"dataset-churn-whnull.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = pd.read_csv(path_data+\"dataset-churn-whnull.csv\")\n",
    "#df = changeTypeCol(df)\n",
    "\n",
    "miss = assess_NA(df)\n",
    "miss.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check duplicated data\n",
    "if len(df[df.duplicated()]) > 0:\n",
    "    print(\"len of duplicated entries: \", len(df[df.duplicated()]))\n",
    "    print(df[df.duplicated(keep=False)].sort_values(by=list(df.columns)).head())\n",
    "else:\n",
    "    print(\"No duplicated entries found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution categorical data\n",
    "This step is important to have an ideas about the unique values of each categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dist_cat(df, col):\n",
    "    plt.figure(figsize=(8,4))\n",
    "    count  = df.value_counts()\n",
    "    g = sns.barplot(count.index, count.values, alpha=0.8)\n",
    "\n",
    "    plt.title('Distribution '+col) \n",
    "    plt.ylabel(\"Number of Occurence\", fontsize=12)\n",
    "    plt.xlabel(col, fontsize=12)\n",
    "    sns.despine()\n",
    "    \n",
    "cat = [dist_cat(df[col], col) for col in df.select_dtypes(include='category').columns]\n",
    "#scatter = pd.scatter_matrix(X_train, c= y_train, marker = 'o', s=40, hist_kwds={'bins':15}, figsize=(9,9), cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cat = list(df.select_dtypes(include='category').columns.values)\n",
    "\n",
    "bool_cat = ['Partner', 'Dependents', 'PhoneService','PaperlessBilling', 'Churn']\n",
    "for c in bool_cat:\n",
    "    df[c] = df[c].map({'Yes':1, 'No':0})\n",
    "df[\"MultipleLines\"] = df[\"MultipleLines\"].map({'Yes':1, 'No':0, 'No phone service':0})\n",
    "df[\"gender\"] = df[\"gender\"].map({'Male':1, 'Female':0})\n",
    "df[\"InternetService\"] = df[\"InternetService\"].map({'No':0, 'DSL':1, 'Fiber optic':2})\n",
    "df[\"OnlineSecurity\"] = df[\"OnlineSecurity\"].map({'No':0, 'No internet service':0,'Yes':1})\n",
    "df[\"DeviceProtection\"] = df[\"DeviceProtection\"].map({'No':0, 'No internet service':0,'Yes':1})\n",
    "df[\"TechSupport\"] = df[\"TechSupport\"].map({'No':0, 'No internet service':0,'Yes':1})\n",
    "df[\"StreamingTV\"] = df[\"StreamingTV\"].map({'No':0, 'No internet service':0,'Yes':1})\n",
    "df[\"StreamingMovies\"] = df[\"StreamingMovies\"].map({'No':0, 'No internet service':0,'Yes':1})\n",
    "df[\"Contract\"] = df[\"Contract\"].map({'Month-to-month':0, 'One year':1,'Two year':2})\n",
    "df[\"PaymentMethod\"] = df[\"PaymentMethod\"].map({'Electronic check':0, 'Mailed check':1, \n",
    "                                               'Bank transfer (automatic)':2,'Credit card (automatic)':3})\n",
    "df[\"OnlineBackup\"] = df[\"OnlineBackup\"].map({'No':0, 'No internet service':0, 'Yes':1})\n",
    "\n",
    "print(df.isnull().sum())\n",
    "\n",
    "df.to_csv(path_data+\"dataset-churn-mapped.csv\", index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data mapped\n",
    "df = pd.read_csv(path_data+\"dataset-churn-mapped.csv\")\n",
    "print(df.isnull().sum())\n",
    "df.head()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation categorical data between the label churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pourcentDiff(distr_churn0, distr_churn1):\n",
    "    distr_churn1[\"percent\"] = None\n",
    "    distr_churn0[\"percent\"] = None\n",
    "    for j in range(len(distr_churn0)):\n",
    "        if (distr_churn1[\"Churn\"][j] < distr_churn0[\"Churn\"][j]):\n",
    "            distr_churn1[\"percent\"][j] = distr_churn0[\"percent\"][j] = distr_churn1[\"Churn\"][j]*100/distr_churn0[\"Churn\"][j]\n",
    "        else:\n",
    "            distr_churn1[\"percent\"][j] = distr_churn0[\"percent\"][j] = distr_churn0[\"Churn\"][j]*100/distr_churn1[\"Churn\"][j]\n",
    "    return distr_churn0, distr_churn1\n",
    "\n",
    "def piechart(distr_churn0):\n",
    "    slices = distr_churn0[\"percent\"].values\n",
    "    classe = range(len(distr_churn0[\"percent\"]))\n",
    "\n",
    "    plt.pie(slices,\n",
    "            labels=classe,\n",
    "            startangle=90,\n",
    "            shadow= True,\n",
    "            autopct='%1.1f%%'\n",
    "            )\n",
    "\n",
    "#electronic check\n",
    "def distr_bet_churn(df, col):\n",
    "    df_no_churn = df.loc[df[\"Churn\"]==0]\n",
    "    df_churn = df.loc[df[\"Churn\"]==1]\n",
    "    distr_churn0 = df_no_churn[[col, \"Churn\"]].groupby(col).count()\n",
    "    distr_churn1 = df_churn[[col, \"Churn\"]].groupby(col).count()\n",
    "    distr_churn0, distr_churn1 = pourcentDiff(distr_churn0, distr_churn1)\n",
    "    plt.figure(figsize=(11,5))\n",
    "    plt.subplot(121)\n",
    "    plt.tight_layout()\n",
    "    # plot our bar chart in a 1er subplot\n",
    "    plt.bar(distr_churn0[\"Churn\"].index - 0.25/2, distr_churn0[\"Churn\"].values, color = '#008E8E', width = 0.25)\n",
    "    plt.bar(distr_churn0[\"Churn\"].index + 0.25/2, distr_churn1[\"Churn\"].values, color = '#DB1702', width = 0.25)\n",
    "    plt.title(\"Relation \"+col+\" and Churn\")\n",
    "    plt.xlabel(col)\n",
    "    plt.legend([\"No Churn\", \"Churn\"])\n",
    "    plt.ylabel(\"freq\")\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "    plt.xticks(range(len(distr_churn0)))\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    # plot our pie chart in a 2th subplot\n",
    "    plt.tight_layout()\n",
    "    piechart(distr_churn0)\n",
    "    plt.title(\"Customer churn (%) between \"+col)\n",
    "    \n",
    "    if col==\"PaymentMethod\":\n",
    "        plt.legend([\"Electronic\", \"Mailed\", \"Bank transfer\", \"Credit Card\"])\n",
    "        \n",
    "    elif col==\"Contract\":\n",
    "        plt.legend([\"Month-to-Month\", \"One year\", \"Two year\"])\n",
    "    \n",
    "    else:\n",
    "        plt.legend([\"no use \"+col, \"use \"+col])\n",
    "    \n",
    "    \n",
    "    \n",
    "list_cat = all_cat\n",
    "list_cat.remove(\"Churn\")\n",
    "list_cat.remove('SeniorCitizen')\n",
    "distr_ch = [distr_bet_churn(df, col) for col in list_cat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate plots (Numerical variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dist_num(df, col):\n",
    "    plt.figure()\n",
    "    sns.distplot(df, kde=False, bins=20);\n",
    "    plt.title(\"Distribution of \"+str(col))\n",
    "    plt.ylabel(\"freq\")\n",
    "    sns.despine()\n",
    "col_num = [\"tenure\", \"MonthlyCharges\", \"TotalCharges\"]\n",
    "\n",
    "num = [dist_num(df[col], col) for col in col_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = [\"tenure\", \"MonthlyCharges\", \"TotalCharges\", \"Churn\"]\n",
    "sns.pairplot(df[col], hue=\"Churn\", size=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check outlier Numerical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot = df.boxplot(figsize=(15,6), column=['tenure', 'MonthlyCharges'], by=\"Churn\", layout=(1, 2))\n",
    "boxplot = df.boxplot(figsize=(15,6), column=['TotalCharges'], by=\"Churn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Capping the outlier rows with Percentiles\n",
    "def detOutlier(data, column):\n",
    "\n",
    "    Q1_Income = data[column].quantile(0.25)\n",
    "    Q3_Income = data[column].quantile(0.75)\n",
    "    IQR_Income = Q3_Income  - Q1_Income\n",
    "    upper_Income = Q3_Income + 1.5 * IQR_Income\n",
    "    lower_Income = Q1_Income - 1.5 * IQR_Income\n",
    "    \n",
    "    data[column].loc[(data[column] > upper_Income)] = np.median(data[column]) \n",
    "    data[column].loc[(data[column] < lower_Income)] = np.median(data[column])\n",
    "    \n",
    "    return data\n",
    "print(df[\"TotalCharges\"].sum())\n",
    "outlier = [\"TotalCharges\", \"tenure\"]\n",
    "for c in outlier:\n",
    "    data_train1 = detOutlier(df[df[\"Churn\"]==0], c)\n",
    "    df = data_train1.append(detOutlier(df[df[\"Churn\"]==1], c))\n",
    "print(df[\"TotalCharges\"].sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change the outlier in a median value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Capping the outlier rows with Percentiles\n",
    "def detOutlier(data, column, target):\n",
    "    \n",
    "    \n",
    "    Q1_Income = data[column].loc[data[column]==target].quantile(0.25)\n",
    "    Q3_Income = data[column].loc[data[column]==target].quantile(0.75)\n",
    "    IQR_Income = Q3_Income - Q1_Income\n",
    "    upper_Income = Q3_Income + 1.5 * IQR_Income\n",
    "    lower_Income = Q1_Income - 1.5 * IQR_Income\n",
    "    \n",
    "    data[column].loc[(data[column] > upper_Income) & (data[\"Churn\"]==target)] = np.median(data[column].loc[data[\"Churn\"]==target]) \n",
    "    data[column].loc[(data[column] < lower_Income) & (data[\"Churn\"]==target)] = np.median(data[column].loc[data[\"Churn\"]==target])\n",
    "    \n",
    "    return data\n",
    "\n",
    "print(df[\"TotalCharges\"].loc[df[\"Churn\"]==1].mean())\n",
    "      \n",
    "#data = detOutlier(df, \"TotalCharges\", 0) \n",
    "data = detOutlier(df, \"TotalCharges\", 1) \n",
    "\n",
    "print(data[\"TotalCharges\"].loc[data[\"Churn\"]==1].mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "corr = df.corr(method='pearson')\n",
    "\n",
    "sns.heatmap(corr, annot=True, cmap=plt.cm.Reds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "df = pd.read_csv(path_data+\"dataset-churn-mapped.csv\")\n",
    "\n",
    "model = ExtraTreesClassifier()\n",
    "\n",
    "col_features = list(df.columns[1:-1].values)\n",
    "\n",
    "X = df[col_features]\n",
    "\n",
    "y = df[\"Churn\"]\n",
    "\n",
    "model = model.fit(X, y)\n",
    "\n",
    "# display the relative importance of each attribute\n",
    "importances = model.feature_importances_\n",
    "\n",
    "plt.barh(col_features, importances, color=\"r\",align=\"center\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, normalize, scale\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV, KFold, cross_val_score\n",
    "from sklearn import preprocessing\n",
    "import pickle\n",
    "from sklearn.utils import shuffle\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import auc, accuracy_score, confusion_matrix, mean_squared_error\n",
    "\n",
    "\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "colUsed = [\"TotalCharges\", \"MonthlyCharges\", \"PaymentMethod\", \"PaperlessBilling\", \"Contract\", \"StreamingMovies\",\n",
    "            \"StreamingTV\", \"TechSupport\", \"DeviceProtection\", \"OnlineSecurity\", \"InternetService\", \"MultipleLines\",\n",
    "            \"PhoneService\", \"tenure\", \"Dependents\", \"Partner\", \"SeniorCitizen\", \"gender\"]\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X[colUsed], y, test_size=0.3, random_state=None)\n",
    "print(\"data splited\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(learning_rate= 0.01, eta= 0.01, max_depth= 5, #objective=\"multi:softprob\", \n",
    "                              n_estimators= 125, min_child_weight= 7 ,# subsample= 0.8, colsample_bytree= 0.9,\n",
    "#                             #gamma=0.3)\n",
    "                             )\n",
    "                              \n",
    "xgb_model.fit(x_train, y_train)\n",
    "                              \n",
    "y_pred_train = xgb_model.predict(x_train)\n",
    "                              \n",
    "y_pred_test = xgb_model.predict(x_test)\n",
    "                              \n",
    "print(\"accuracy xgboost TR:\",np.round(accuracy_score(y_train, y_pred_train), 3))\n",
    "print(\"accuracy xgboost TS:\",np.round(accuracy_score(y_test, y_pred_test), 3))\n",
    "                              \n",
    "plot_confusion_matrix(y_train, y_pred_train,classes=[0, 1],\n",
    "                      title='Confusion matrix, train', normalize=True)\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred_test,classes=[0, 1],\n",
    "                      title='Confusion matrix, test', normalize=True)                              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "grid_params = {'lr':[0.01],\n",
    "               'min_child_weight':[5,7,10,15],\n",
    "               'max_depth':[3],\n",
    "               'n_estimators':[100, 125, 150]}\n",
    "gs = GridSearchCV(\n",
    "\t              xgb.XGBClassifier(),\n",
    "\t              grid_params,\n",
    "\t              verbose=1,\n",
    "\t              cv=5,\n",
    "\t              n_jobs=-1)\n",
    "from sklearn.metrics import auc, accuracy_score, confusion_matrix, mean_squared_error\n",
    "\n",
    "res = gs.fit(df[colUsed], df[\"Churn\"])\n",
    "\n",
    "print(res.best_score_)\n",
    "print(res.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = RandomForestClassifier(n_estimators=200, max_depth=15, random_state=0, criterion='gini')\n",
    "                              \n",
    "xgb_model.fit(x_train, y_train)\n",
    "                              \n",
    "y_pred_train = xgb_model.predict(x_train)\n",
    "                              \n",
    "y_pred_test = xgb_model.predict(x_test)\n",
    "                              \n",
    "print(\"accuracy xgboost TR:\",np.round(accuracy_score(y_train, y_pred_train), 3))\n",
    "print(\"accuracy xgboost TS:\",np.round(accuracy_score(y_test, y_pred_test), 3))\n",
    "                              \n",
    "plot_confusion_matrix(y_train, y_pred_train,classes=[0, 1],\n",
    "                      title='Confusion matrix, train', normalize=True)\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred_test,classes=[0, 1],\n",
    "                      title='Confusion matrix, test', normalize=True)              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "df = shuffle(df)\n",
    "\n",
    "colUsed = [\"TotalCharges\", \"MonthlyCharges\", \"PaymentMethod\", \"PaperlessBilling\", \"Contract\", \"StreamingMovies\",\n",
    "            \"StreamingTV\", \"TechSupport\", \"DeviceProtection\", \"OnlineSecurity\", \"InternetService\", \"MultipleLines\",\n",
    "            \"PhoneService\", \"tenure\", \"Dependents\", \"Partner\", \"SeniorCitizen\", \"gender\"]\n",
    "\n",
    "smote = SMOTE('minority')\n",
    "\n",
    "x_data, y_data = smote.fit_sample(df[colUsed], df[\"Churn\"])\n",
    "x_data.shape, y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.3, random_state=None)\n",
    "print(\"data splited\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ss = StandardScaler()\n",
    "\n",
    "#x_train = ss.fit_transform(x_train)\n",
    "\n",
    "#x_test = ss.transform(x_test)\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(learning_rate= 0.1, eta= 0.01, max_depth= 7, #objective=\"multi:softprob\", \n",
    "                              n_estimators= 125, min_child_weight= 3 , #subsample= 0.6, colsample_bytree= 0.9,\n",
    "#                             #gamma=0.3)\n",
    "                             )\n",
    "\n",
    "#xgb_model = RandomForestClassifier(n_estimators=150, max_depth=30, random_state=0, criterion='gini')\n",
    "                              \n",
    "#xgb_model = DummyClassifier(strategy='most_frequent') \n",
    "    \n",
    "xgb_model.fit(x_train, y_train)\n",
    "                              \n",
    "y_pred_train = xgb_model.predict(x_train)\n",
    "                              \n",
    "y_pred_test = xgb_model.predict(x_test)\n",
    "                              \n",
    "print(\"accuracy xgboost TR:\",np.round(accuracy_score(y_train, y_pred_train), 3))\n",
    "print(\"accuracy xgboost TS:\",np.round(accuracy_score(y_test, y_pred_test), 3))\n",
    "                              \n",
    "plot_confusion_matrix(y_train, y_pred_train,classes=[0, 1],\n",
    "                      title='Confusion matrix, train', normalize=True)\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred_test,classes=[0, 1],\n",
    "                      title='Confusion matrix, test', normalize=True)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "clf_stump = xgb.XGBClassifier(learning_rate= 0.1, eta= 0.01, max_depth= 7, #objective=\"multi:softprob\", \n",
    "                              n_estimators= 125, min_child_weight= 3 , #subsample= 0.6, colsample_bytree= 0.9,\n",
    "#                             #gamma=0.3)\n",
    "                             )\n",
    "\n",
    "\n",
    "xgb_model= BaggingClassifier(base_estimator=clf_stump, n_estimators=100, max_samples=0.35)\n",
    "\n",
    "xgb_model.fit(x_train, y_train)\n",
    "                              \n",
    "y_pred_train = xgb_model.predict(x_train)\n",
    "                              \n",
    "y_pred_test = xgb_model.predict(x_test)\n",
    "                              \n",
    "print(\"accuracy xgboost TR:\",np.round(accuracy_score(y_train, y_pred_train), 3))\n",
    "print(\"accuracy xgboost TS:\",np.round(accuracy_score(y_test, y_pred_test), 3))\n",
    "                              \n",
    "plot_confusion_matrix(y_train, y_pred_train,classes=[0, 1],\n",
    "                      title='Confusion matrix, train', normalize=True)\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred_test,classes=[0, 1],\n",
    "                      title='Confusion matrix, test', normalize=True)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
